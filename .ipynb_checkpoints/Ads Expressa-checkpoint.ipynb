{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_xpath_exist(xpath):\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def scrape(links):\n",
    "    data = pd.DataFrame(columns=['Topic', 'Body', 'in_post_url','Additional Info', 'facebook_url', 'Likes', 'Comments', 'Shares']) \n",
    "    currUrl = driver.current_url\n",
    "    \n",
    "    for link in links:\n",
    "        driver.get(link)\n",
    "    \n",
    "        soup1 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        body = soup1.find('div', {'class':'ads-info'}).text\n",
    "        additional_info = soup1.find('ul', {'class':'list-custom'}).text\n",
    "        \n",
    "\n",
    "        topic = soup1.find(class_='entry-title').get_text()\n",
    "\n",
    "        newUrl = soup1.find('iframe', {'allow': 'encrypted-media'})['src']\n",
    "        driver.get(newUrl)\n",
    "\n",
    "        soup2=BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        like = soup2.find('span', {'class':'embeddedLikeButton'}).get_text()\n",
    "        comment = soup2.find('div', {'title':'Comment'}).get_text()\n",
    "        share = soup2.find('div', {'title':'Share'}).get_text()\n",
    "        \n",
    "        try:\n",
    "            in_post_link = soup1.find('div', {'class':'ads-info'}).find('a')['href']\n",
    "        except:\n",
    "            in_post_link = \"\"\n",
    "\n",
    "        data = data.append({'Topic':topic, 'Body': body, 'in_post_url': in_post_link, 'Additional Info': additional_info, 'facebook_url':link, 'Likes':like, 'Comments':comment, 'Shares':share}, ignore_index=True)  \n",
    "    \n",
    "    driver.get(currUrl)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Remote(command_executor='http://127.0.0.1:9515', desired_capabilities=DesiredCapabilities.CHROME)\n",
    "\n",
    "driver.get(\"https://adespresso.com/ads-examples/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    driver.find_element_by_name('firstname').send_keys(\"Daniel\")\n",
    "    driver.find_element_by_name('email').send_keys(\"daniel@gmail.com\")\n",
    "    driver.find_element_by_xpath(\"//input[@value='Count me In!']\").click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# search = input(\"Search For:\")\n",
    "# driver.find_element_by_id('ads-search').send_keys(search)\n",
    "# driver.find_element_by_id('ads-search').send_keys(Keys.ENTER)\n",
    "\n",
    "load_older = \"//a[contains(text(),'Older Ads')]\"\n",
    "\n",
    "\n",
    "page_num = 0\n",
    "\n",
    "while check_xpath_exist(load_older):\n",
    "    try:\n",
    "        print(\"Scraping page {}\".format(str(page_num)))\n",
    "        initialSoup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        links = initialSoup.find_all('li', {'class': 'sf-item'})\n",
    "        links = list(set([link.find('a')['href'] for link in links]))\n",
    "\n",
    "        soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "        df = scrape(links)\n",
    "\n",
    "        if page_num == 0:\n",
    "            df.to_csv('data.csv', index=False)\n",
    "        else:\n",
    "            df.to_csv('data.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        page_num = page_num + 1\n",
    "        driver.find_element_by_xpath(load_older).click()\n",
    "    except Exception as e:\n",
    "        print(\"Got an error: {}\".format(str(e)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
